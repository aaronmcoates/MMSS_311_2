setwd("~/Documents/GitHub/MMSS_311_2")
pol <- read.csv("/Users/aaroncoates/Downloads/pol_data.csv")
library(broom)
library(e1071)
library(caret)
pol$group <- as.factor(pol$group)
trainsize <- floor((2/3)*nrow(pol))
set.seed(100)
train_pol <- sample(nrow(pol), size = trainsize, replace=FALSE)
trainingdata <- pol[train_pol, ]
testydata <- pol[-train_pol, ]
tunez <- tune(svm, group ~ pol_margin + col_degree + house_income,
data=trainingdata, kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
swaggysvm <- tunez$best.model
summary(swaggysvm)
svmpredict <- predict(swaggysvm, testydata)
svmtable <- table("Prediction"=svmpredict, "True Party"=testydata[,1])
svmtable
naive <- naiveBayes(group ~ pol_margin + col_degree + house_income, data=trainingdata)
naivetidy <- tidy(naive)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/MMSS_311_2")
pol <- read.csv("/Users/aaroncoates/Downloads/pol_data.csv")
library(broom)
library(e1071)
library(caret)
pol$group <- as.factor(pol$group)
trainsize <- floor((2/3)*nrow(pol))
set.seed(100)
train_pol <- sample(nrow(pol), size = trainsize, replace=FALSE)
trainingdata <- pol[train_pol, ]
testydata <- pol[-train_pol, ]
tunez <- tune(svm, group ~ pol_margin + col_degree + house_income,
data=trainingdata, kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
swaggysvm <- tunez$best.model
summary(swaggysvm)
svmpredict <- predict(swaggysvm, testydata)
svmtable <- table("Prediction"=svmpredict, "True Party"=testydata[,1])
svmtable
naive <- naiveBayes(group ~ pol_margin + col_degree + house_income, data=trainingdata)
naivepred <- predict(naive, testydata)
naivetable <- table("Prediction"=naivepred, "True Party"=testydata[,1])
naivetable
View(naive)
View(naive)
setwd("~/Documents/GitHub/MMSS_311_2")
library(xml2)
library(rvest)
setwd("~/Documents/GitHub/MMSS_311_2")
library(xml2)
library(rvest)
setwd("~/Documents/GitHub/MMSS_311_2")
library(xml2)
library(rvest)
setwd("~/Documents/GitHub/MMSS_311_2")
library(xml2)
library(rvest)
setwd("~/Documents/GitHub/MMSS_311_2")
library(xml2)
library(rvest)
hello <- read_html('/Users/aaroncoates/Desktop/cries.html', skip=0)
nodes <- html_nodes(hello, 'li a')
text <- html_text(nodes)
hrefy <- html_attr(nodes, "href")
hrefy
hrefy <- html_attr(nodes)
hrefy <- html_attr(nodes, 'url')
hrefy
hrefy <- html_attr(nodes, 'class')
hrefy
hrefy <- html_attr(nodes, "href")
hrefy
nodes <- html_nodes(hello, 'ul li')
text <- html_text(nodes)
hrefy <- html_attr(nodes, "href")
hrefy
nodes <- html_nodes(hello, 'li a')
text <- html_text(nodes)
hrefy <- html_attr(nodes, "href")
hrefy
hrefy
hello <- read_html('/Users/aaroncoates/Desktop/cries.html', skip=0)
no <- read_html('/Users/aaroncoates/Desktop/cries.html', skip=0)
nodes <- html_nodes(no, 'li a')
text <- html_text(nodes)
hrefy <- html_attr(nodes, "href")
hrefy
text <- html_text(nodes)
hrefy <- html_attr(nodes, "href")
hrefy
hrefy <- html_attr(nodes, "a href")
hrefy
hrefy <- html_attr(nodes, "href")
hrefy2 <- url_absolute(hrefy)
hrefy2 <- url_absolute(hrefy, 'https://en.wikipedia.org/wiki/Category:Member_states_of_the_Association_of_Southeast_Asian_Nations')
hrefy2
papi <- cbind(text, href)
slay <- as.data.frame(papi)
newframe <- slay[3:12,]
papi <- cbind(text, hrefy2)
slay <- as.data.frame(papi)
newframe <- slay[3:12,]
View(newframe)
read_html(newframe$hrefy)
read_html(slay$hrefy2)
read_html(newframe$hrefy2)
read_html(newframe$hrefy2)
read_html(newframe$hrefy2)
read_html('newframe$hrefy2')
read_html(newframe$hrefy2)
read_html(newframe$hrefy2)
read_html(newframe$hrefy2, stringsAsFactors=FALSE)
newframe <- newframe(stringsAsFactors=FALSE)
newframe <- newframe[stringsAsFactors=FALSE]
slay <- as.data.frame(papi, stringsAsFactors = F)
newframe <- slay[3:12,]
read_html(newframe$hrefy2)
read_html(newframe$hrefy2)
papi <- cbind(text, hrefy2)
slay <- as.data.frame(papi, stringsAsFactors = F)
newframe <- slay[3:12,]
read_html(newframe$hrefy2)
read_html(newframe[1,2])
ah <- read_html(newframe[1,2])
ah
for(i in 1:10) {
read_html(newframe[i,2])}
for(i in 1:10) {
read_html(newframe[i,2]) %>%
html_nodes('div p') %>%
html_text()}
hi <- for(i in 1:10) {
read_html(newframe[i,2]) %>%
html_nodes('div p') %>%
html_text()}
hi
hi <- for(i in 1:10) {
read_html(newframe[i,2]) %>%
html_nodes('div p') %>%
html_text()}
hi <- newframe$hrefy2[1] %>%
read_html() %>%
html_nodes('div p') %>%
html_text()
hi
hi <- newframe$hrefy2[1] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
. [1:10]
hi
hi <- newframe$hrefy2[1] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
hi <- newframe$hrefy2[1] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
hi
hi <- newframe$hrefy2[1] %>%
read_html() %>%
html_nodes('div p') %>%
html_text()
hi
hi %>% paste(collapse = '\n\n')
hi
hi <- newframe$hrefy2[1] %>%
read_html() %>%
html_nodes('div p') %>%
html_text()
hi
hi
hi <- newframe$hrefy2[1] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')
hi
hi <- for(i in 1:nrow(newframe))
{newframe$hrefy2[i] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')}
hi <- for(i in 1:nrow(newframe)){
newframe$hrefy2[i] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')}
hi <- for(i in 1:nrow(newframe)){
newframe$hrefy2[i] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')
}
hi <- for(i in 1:nrow(newframe)){
newframe$hrefy2[i] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')
}
hi <- for(i in 1:nrow(newframe)){
newframe$hrefy2[i] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')
}
hi <- for(i in 1:10){
newframe$hrefy2[i] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')
}
hi <- newframe$hrefy2[1] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')
hi <- newframe$hrefy2[2] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')
hi
hi <- for(i in 1:10){
newframe$text[i] <- newframe$hrefy2[i] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')
}
hi <- for(i in 1:10){
newframe$text[i] <- newframe$hrefy2[i] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')
}
View(newframe)
hi <- newframe$hrefy2[2] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')
View(newframe)
setwd("~/Documents/GitHub/MMSS_311_2")
library(xml2)
library(rvest)
no <- read_html('/Users/aaroncoates/Desktop/cries.html', skip=0)
nodes <- html_nodes(no, 'li a')
text <- html_text(nodes)
hrefy <- html_attr(nodes, "href")
hrefy2 <- url_absolute(hrefy, 'https://en.wikipedia.org/wiki/Category:Member_states_of_the_Association_of_Southeast_Asian_Nations')
hrefy2
papi <- cbind(text, hrefy2)
slay <- as.data.frame(papi, stringsAsFactors = F)
newframe <- slay[3:12,]
hi <- for(i in 1:10){
newframe$whyohwhy[i] <- newframe$hrefy2[i] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')
}
View(newframe)
newframe <- slay[3:12,]
setwd("~/Documents/GitHub/MMSS_311_2")
library(xml2)
library(rvest)
no <- read_html('/Users/aaroncoates/Desktop/cries.html', skip=0)
nodes <- html_nodes(no, 'li a')
text <- html_text(nodes)
hrefy <- html_attr(nodes, "href")
hrefy2 <- url_absolute(hrefy, 'https://en.wikipedia.org/wiki/Category:Member_states_of_the_Association_of_Southeast_Asian_Nations')
hrefy2
papi <- cbind(text, hrefy2)
slay <- as.data.frame(papi, stringsAsFactors = F)
newframe <- slay[3:12,]
for(i in 1:10){
newframe$whyohwhy[i] <- newframe$hrefy2[i] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')
}
setwd("~/Documents/GitHub/MMSS_311_2")
library(xml2)
library(rvest)
no <- read_html('/Users/aaroncoates/Desktop/cries.html', skip=0)
nodes <- html_nodes(no, 'li a')
text <- html_text(nodes)
hrefy <- html_attr(nodes, "href")
hrefy2 <- url_absolute(hrefy, 'https://en.wikipedia.org/wiki/Category:Member_states_of_the_Association_of_Southeast_Asian_Nations')
hrefy2
papi <- cbind(text, hrefy2)
slay <- as.data.frame(papi, stringsAsFactors = F)
newframe <- slay[3:12,]
for(i in 1:10){
newframe$whyohwhy[i] <- newframe$hrefy2[i] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')
}
setwd("~/Documents/GitHub/MMSS_311_2")
library(xml2)
library(rvest)
no <- read_html('/Users/aaroncoates/Desktop/cries.html', skip=0)
nodes <- html_nodes(no, 'li a')
text <- html_text(nodes)
hrefy <- html_attr(nodes, "href")
hrefy2 <- url_absolute(hrefy, 'https://en.wikipedia.org/wiki/Category:Member_states_of_the_Association_of_Southeast_Asian_Nations')
hrefy2
papi <- cbind(text, hrefy2)
slay <- as.data.frame(papi, stringsAsFactors = F)
newframe <- slay[3:12,]
for(i in 1:10){
newframe$whyohwhy[i] <- newframe$hrefy2[i] %>%
read_html() %>%
html_nodes('div p') %>%
html_text() %>%
paste(collapse = '\n')
}
read.csv(/Users/aaroncoates/Downloads/sick_data.csv)
read.csv(Users/aaroncoates/Downloads/sick_data.csv)
read.csv(~Downloads/sick_data.csv)
install.packages('tidytext')
library(tidytext)
corpus
install.packages('tm')
library(tm)
milk <- read.csv(/Users/aaroncoates/Downloads/sick_data.csv)
milk <- read.csv('/Users/aaroncoates/Downloads/sick_data.csv')
View(milk)
milk <- read.csv('/Users/aaroncoates/Downloads/trumptweets.csv')
View(milk)
heysis <- milk$text %>%
unnest_tokens(word, text)
baddiecorpusiii <- Corpus(VectorSource(as.vector(milk$text)))
baddiecorpusiii
baddiecorpusiii %>%
tm_map(removeWords, stopwords("english"))
baddiecorpusiii
baddiecorpusiii <- Corpus(VectorSource(as.vector(milk$text)))
baddiecorpusiii %>%
count(word) %>%
arrange(desc(n))
baddiecorpusiii %>%
tally(word) %>%
arrange(desc(n))
baddiecorpusiii %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english")
View(baddiecorpusiii)
baddiecorpusiii %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation)) %>%
tm_map(removeSparseTerms(.99))
baddiecorpusiii %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation)) %>%
tm_map(content_transformer(removeSparseTerms(.99)))
baddiecorpusiii <- Corpus(VectorSource(as.vector(milk$text)))
baddiecorpusiii %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation))
baddiecorpusiii
baddiecorpusiii %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation)) %>%
tm_map(stemDocument, language = "english")
baddiecorpusiii %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation)) %>%
tm_map(stemDocument, language = "english") %>%
as.DocumentTermMatrix()
ahh <- TermDocumentMatrix(baddiecorpusiii)
ahh
removeSparseTerms(ahh, .99)
ahh2 <- removeSparseTerms(ahh, .99)
ahh <- TermDocumentMatrix(baddiecorpusiii)
ahh2 <- removeSparseTerms(ahh, .99)
View(ahh2)
baddiecorpusiii <- Corpus(VectorSource(as.vector(milk$text)))
baddiecorpusiii %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation)) %>%
tm_map(stemDocument, language = "english")
tokenize(baddiecorpusiii)
corpusiii <- Corpus(VectorSource(as.vector(milk$text)))
corpusiii %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation)) %>%
tm_map(stemDocument, language = "english")
ahh <- TermDocumentMatrix(baddiecorpusiii)
ahh2 <- removeSparseTerms(ahh, .99)
View(ahh2)
ahh <- DocumentTermMatrix(baddiecorpusiii)
ahh2 <- removeSparseTerms(ahh, .99)
inspect(ahh2[1:5, 1:5])
corpusiii <- Corpus(VectorSource(as.vector(milk$text)))
corpusiii %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation)) %>%
tm_map(stemDocument, language = "english")
ahh <- DocumentTermMatrix(corpusiii)
ahh2 <- removeSparseTerms(ahh, .99)
inspect(ahh2[1:5, 1:5])
inspect(ahh2[1:10, 1:10])
corpusiii[1:5,1:5]
corpusiii[1:5]
corpusiii <- Corpus(VectorSource(as.vector(milk$text)))
corpusiii %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation)) %>%
tm_map(stemDocument, language = "english")
ahh <- DocumentTermMatrix(corpusiii)
ahh2 <- removeSparseTerms(ahh, .99)
corpusiii <- Corpus(VectorSource(as.vector(milk$text)))
corpusiii %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation))
ahh <- DocumentTermMatrix(corpusiii)
ahh2 <- removeSparseTerms(ahh, .99)
inspect(ahh2[1:5,1:5])
corpusiii %>%
tm_map(content_transformer(removeWords, stopwords("english"))) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation))
ahh <- DocumentTermMatrix(corpusiii)
ahh2 <- removeSparseTerms(ahh, .99)
inspect(ahh2[1:5,1:5])
corpusiii %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation))
ahh <- DocumentTermMatrix(corpusiii)
ahh2 <- removeSparseTerms(ahh, .99)
inspect(ahh2[1:5,1:5])
corpusiii <- Corpus(VectorSource(as.vector(milk$text)))
corpusiii %>%
tm_map(removeWords, stopwords("en")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation))
ahh <- DocumentTermMatrix(corpusiii)
ahh2 <- removeSparseTerms(ahh, .99)
inspect(ahh2[1:5,1:5])
library(dplyr)
corpusiv <- corpusiii %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation))
ahh <- DocumentTermMatrix(corpusiv)
ahh2 <- removeSparseTerms(ahh, .99)
inspect(ahh2[1:5,1:5])
library(broom)
tidy(broom)
tidy(ahh2)
ahh2 <- tidy(ahh2)
View(ahh2)
ahh2 <- as.matrix(ahh2)
View(ahh2)
corpusiii <- Corpus(VectorSource(as.vector(milk$text)))
corpusiv <- corpusiii %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation))
ahh <- DocumentTermMatrix(corpusiv)
ahh2 <- removeSparseTerms(ahh, .99)
inspect(ahh2[1:5,1:5])
TfIdf(ahh2)
tidy(ahh2)
bind_tf_idf(ahh2)
bind_tf_idf(ahh2, term, document, count)
ahh4 <- DocumentTermMatrix(ahh3, control = list(weighting = weightTfIdf))
ahh3 <- tidy(ahh2)
ahh4 <- DocumentTermMatrix(ahh3, control = list(weighting = weightTfIdf))
ahh4 <- DocumentTermMatrix(corpusiv, control = list(weighting = weightTfIdf))
