scale_y_continuous(breaks = seq(0:15))
ploty
grid=10^seq(2,-2,length=100)
ridge <- glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=0, lambda=grid)
ridge1 <- tidy(ridge)
ggplot(data=ridge1, aes(x=log10(lambda), y=estimate, colour=term)) +
geom_line() + labs(title ="Ridge Coefficients",
x = "Log(Lambda)", y = "Coefficient Estimate") +
coord_cartesian(xlim = c(-2,2), ylim = c(-1, 1))
crossval <- cv.glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=0, lambda=grid)
plot(crossval)
crossval$lambda.min
coef(crossval, s = "lambda.min")
grid=10^seq(2,-2,length=100)
lasso <- glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=1, lambda=grid)
lasso1 <- tidy(lasso)
ggplot(data=lasso1, aes(x=lambda, y=estimate, colour=term)) +
geom_line() + coord_cartesian(xlim = c(0,2), ylim = c(-1, 1)) +
labs(title ="LASSO Coefficients", x = "Lambda", y = "Coefficient Estimate")
ggplot(data=lasso1, aes(x=lambda, y=estimate, colour=term)) +
geom_line() + coord_cartesian(xlim = c(0,.5), ylim = c(-.5, .5)) +
labs(title ="LASSO Coefficients", x = "Lambda", y = "Coefficient Estimate")
crossvallasso <- cv.glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=1, lambda=grid)
plot(crossvallasso)
crossvallasso$lambda.min
coef(crossvallasso, s = "lambda.min")
ggplot(data=ridge1, aes(x=log10(lambda), y=estimate, colour=term)) +
geom_line() + labs(title ="Ridge Coefficients",
x = "Log(Lambda)", y = "Coefficient Estimate") +
coord_cartesian(xlim = c(-2,2), ylim = c(-1, 1))
crossval$lambda.min
coef(crossval, s = "lambda.min")
crossvallasso$lambda.min
coef(crossvallasso, s = "lambda.min")
crossvallasso$lambda.min
coef(crossvallasso, s = "lambda.min")
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/MMSS_311_2")
wid <- read.csv("/Users/aaroncoates/Downloads/widget_data.csv")
library(dplyr)
library(ggplot2)
library(glmnet)
library(broom)
library(aod)
ploty <- ggplot(wid, aes(y)) + geom_histogram(binwidth = 1, color="blue", aes(fill = ..x..)) +
labs(title ="Distribution of Widget Performance",
x = "Widget Performance", y = "Frequency") + scale_fill_continuous(guide=FALSE) +
scale_x_continuous(breaks = seq(round(min(wid$y),0), round(max(wid$y),0), by = 1)) +
scale_y_continuous(breaks = seq(0:15))
ploty
grid=10^seq(2,-2,length=100)
ridge <- glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=0, lambda=grid)
ridge1 <- tidy(ridge)
ggplot(data=ridge1, aes(x=log10(lambda), y=estimate, colour=term)) +
geom_line() + labs(title ="Ridge Coefficients",
x = "Log(Lambda)", y = "Coefficient Estimate") +
coord_cartesian(xlim = c(-2,2), ylim = c(-1, 1))
crossval <- cv.glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=0, lambda=grid)
crossval <- cv.glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=0, lambda=grid)
plot(crossval)
crossval$lambda.min
coef(crossval, s = "lambda.min")
crossval$lambda.min
coef(crossval, s = "lambda.min")
grid=10^seq(2,-2,length=100)
lasso <- glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=1, lambda=grid)
lasso1 <- tidy(lasso)
ggplot(data=lasso1, aes(x=lambda, y=estimate, colour=term)) +
geom_line() + coord_cartesian(xlim = c(0,2), ylim = c(-1, 1)) +
labs(title ="LASSO Coefficients", x = "Lambda", y = "Coefficient Estimate")
ggplot(data=lasso1, aes(x=lambda, y=estimate, colour=term)) +
geom_line() + coord_cartesian(xlim = c(0,.5), ylim = c(-.5, .5)) +
labs(title ="LASSO Coefficients", x = "Lambda", y = "Coefficient Estimate")
crossvallasso <- cv.glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=1, lambda=grid)
plot(crossvallasso)
crossvallasso$lambda.min
coef(crossvallasso, s = "lambda.min")
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/MMSS_311_2")
wid <- read.csv("/Users/aaroncoates/Downloads/widget_data.csv")
library(dplyr)
library(ggplot2)
library(glmnet)
library(broom)
library(aod)
ploty <- ggplot(wid, aes(y)) + geom_histogram(binwidth = 1, color="blue", aes(fill = ..x..)) +
labs(title ="Distribution of Widget Performance",
x = "Widget Performance", y = "Frequency") + scale_fill_continuous(guide=FALSE) +
scale_x_continuous(breaks = seq(round(min(wid$y),0), round(max(wid$y),0), by = 1)) +
scale_y_continuous(breaks = seq(0:15))
ploty
grid=10^seq(2,-2,length=100)
ridge <- glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=0, lambda=grid)
ridge1 <- tidy(ridge)
ggplot(data=ridge1, aes(x=log10(lambda), y=estimate, colour=term)) +
geom_line() + labs(title ="Ridge Coefficients",
x = "Log(Lambda)", y = "Coefficient Estimate") +
coord_cartesian(xlim = c(-2,2), ylim = c(-1, 1))
crossval <- cv.glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=0, lambda=grid)
plot(crossval)
crossval$lambda.min
coef(crossval, s = "lambda.min")
grid=10^seq(2,-2,length=100)
lasso <- glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=1, lambda=grid)
lasso1 <- tidy(lasso)
ggplot(data=lasso1, aes(x=lambda, y=estimate, colour=term)) +
geom_line() + coord_cartesian(xlim = c(0,2), ylim = c(-1, 1)) +
labs(title ="LASSO Coefficients", x = "Lambda", y = "Coefficient Estimate")
ggplot(data=lasso1, aes(x=lambda, y=estimate, colour=term)) +
geom_line() + coord_cartesian(xlim = c(0,.5), ylim = c(-.5, .5)) +
labs(title ="LASSO Coefficients", x = "Lambda", y = "Coefficient Estimate")
crossvallasso <- cv.glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=1, lambda=grid)
plot(crossvallasso)
crossvallasso$lambda.min
coef(crossvallasso, s = "lambda.min")
knitr::opts_chunk$set(echo = TRUE)
naive <- naiveBayes(group ~ pol_margin + col_degree + house_income, data=trainingdata)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/MMSS_311_2")
pol <- read.csv("/Users/aaroncoates/Downloads/pol_data.csv")
library(broom)
library(e1071)
library(caret)
pol$group <- as.factor(pol$group)
trainsize <- floor((2/3)*nrow(pol))
set.seed(100)
train_pol <- sample(nrow(pol), size = trainsize, replace=FALSE)
trainingdata <- pol[train_pol, ]
testydata <- pol[-train_pol, ]
tunez <- tune(svm, group ~ pol_margin + col_degree + house_income,
data=trainingdata, kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
swaggysvm <- tunez$best.model
summary(swaggysvm)
svmpredict <- predict(swaggysvm, testydata)
svmtable <- table("Prediction"=svmpredict, "True Party"=testydata[,1])
svmtable
naive <- naiveBayes(group ~ pol_margin + col_degree + house_income, data=trainingdata)
naivetidy <- tidy(naive)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/MMSS_311_2")
pol <- read.csv("/Users/aaroncoates/Downloads/pol_data.csv")
library(broom)
library(e1071)
library(caret)
pol$group <- as.factor(pol$group)
trainsize <- floor((2/3)*nrow(pol))
set.seed(100)
train_pol <- sample(nrow(pol), size = trainsize, replace=FALSE)
trainingdata <- pol[train_pol, ]
testydata <- pol[-train_pol, ]
tunez <- tune(svm, group ~ pol_margin + col_degree + house_income,
data=trainingdata, kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
swaggysvm <- tunez$best.model
summary(swaggysvm)
svmpredict <- predict(swaggysvm, testydata)
svmtable <- table("Prediction"=svmpredict, "True Party"=testydata[,1])
svmtable
naive <- naiveBayes(group ~ pol_margin + col_degree + house_income, data=trainingdata)
naivepred <- predict(naive, testydata)
naivetable <- table("Prediction"=naivepred, "True Party"=testydata[,1])
naivetable
View(naive)
View(naive)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/MMSS_311_2")
library(xml2)
library(rvest)
library(tm, tidytext)
library(stringr)
no <- read_html('/Users/aaroncoates/Desktop/cries.html')
nodes <- html_nodes(no, '.mw-category-group+ .mw-category-group a')
country <- html_text(nodes)
url <- html_attr(nodes, "href")
fullurl <- url_absolute(url, 'https://en.wikipedia.org/wiki/Category:Member_states_of_the_Association_of_Southeast_Asian_Nations')
fullurl
combined <- cbind(country, fullurl)
finaldata <- as.data.frame(combined, stringsAsFactors = F)
country <- html_text(nodes)
url <- html_attr(nodes, "href")
fullurl <- url_absolute(url, 'https://en.wikipedia.org/wiki/Category:Member_states_of_the_Association_of_Southeast_Asian_Nations')
combined <- cbind(country, fullurl)
finaldata <- as.data.frame(combined, stringsAsFactors = F)
finaldata
for(i in 1:10){
finaldata$text[i] <- finaldata$fullurl[i] %>%
read_html() %>%
html_nodes('p+ ul li , p') %>%
html_text() %>%
paste(collapse = ' ')
}
finaldata
for(i in 1:10){
finaldata$text[i] <- finaldata$fullurl[i] %>%
read_html() %>%
html_nodes('p+ ul li , p') %>%
html_text() %>%
paste(collapse = ' ')
}
View(finaldata)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/GitHub/MMSS_311_2")
TwitterData <- read.csv('/Users/aaroncoates/Downloads/trumptweets.csv')
library(tidytext)
library(tm)
library(dplyr)
library(broom)
library(lubridate)
library(stringr)
library(ggplot2)
library(tidyr)
tweetcorpus <- Corpus(VectorSource(as.vector(TwitterData$text)))
processedcorpus <- tweetcorpus %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation))
DTMatrix <- DocumentTermMatrix(processedcorpus)
SparseDTMatrix <- removeSparseTerms(DTMatrix, .99)
inspect(SparseDTMatrix[1:10,1:10])
TfIdfMat <- DocumentTermMatrix(processedcorpus, control
= list(weighting = weightTfIdf))
SparseTfIdfMat <- removeSparseTerms(TfIdfMat, .99)
inspect(SparseTfIdfMat[1:10,1:10])
popterms <- tidymatrix %>%
group_by(term) %>%
summarize(frequency = sum(count)) %>%
arrange(desc(frequency))
tidymatrix <- tidy(PDTMatrix)
DTMatrix <- DocumentTermMatrix(processedcorpus)
SparseDTMatrix <- removeSparseTerms(DTMatrix, .99)
inspect(SparseDTMatrix[1:10,1:10])
tidymatrix <- tidy(PDTMatrix)
tidymatrix <- tidy(PDTMatrix)
tidymatrix <- tidy(DTMatrix)
View(tidymatrix)
TfIdfMat <- DocumentTermMatrix(processedcorpus, control
= list(weighting = weightTfIdf))
SparseTfIdfMat <- removeSparseTerms(TfIdfMat, .99)
inspect(SparseTfIdfMat[1:10,1:10])
popterms <- tidymatrix %>%
group_by(term) %>%
summarize(frequency = sum(count)) %>%
arrange(desc(frequency))
popterms[1:20,1:2]
tweetcorpus <- Corpus(VectorSource(as.vector(TwitterData$text)))
processedcorpus <- tweetcorpus %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation))
DTMatrix <- DocumentTermMatrix(processedcorpus)
SparseDTMatrix <- removeSparseTerms(DTMatrix, .99)
inspect(SparseDTMatrix[1:10,1:10])
tidymatrix <- tidy(DTMatrix)
TfIdfMat <- DocumentTermMatrix(processedcorpus, control
= list(weighting = weightTfIdf))
SparseTfIdfMat <- removeSparseTerms(TfIdfMat, .99)
inspect(SparseTfIdfMat[1:10,1:10])
popterms <- tidymatrix %>%
group_by(term) %>%
summarize(frequency = sum(count)) %>%
arrange(desc(frequency))
popterms[1:20,1:2]
tweetcorpus <- Corpus(VectorSource(as.vector(TwitterData$text)))
processedcorpus <- tweetcorpus %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation))
DTMatrix <- DocumentTermMatrix(processedcorpus)
SparseDTMatrix <- removeSparseTerms(DTMatrix, .99)
inspect(SparseDTMatrix[1:10,1:10])
tidymatrix <- tidy(DTMatrix)
TfIdfMat <- DocumentTermMatrix(processedcorpus, control
= list(weighting = weightTfIdf))
SparseTfIdfMat <- removeSparseTerms(TfIdfMat, .99)
inspect(SparseTfIdfMat[1:10,1:10])
popterms <- tidymatrix %>%
group_by(term) %>%
summarize(frequency = sum(count)) %>%
arrange(desc(frequency))
popterms[1:20,1:2]
TwitterDataDate <- TwitterData
TwitterDataDate$date <- as.Date(TwitterData$created_at, '%m-%d-%Y')
PostTwitterData <- subset(TwitterDataDate, date >= as.Date('2016-11-08'))
PreTwitterData <- subset(TwitterDataDate, date <= as.Date('2016-11-08'))
posttweetcorpus <- Corpus(VectorSource(as.vector(PostTwitterData$text)))
postprocessedcorpus <- posttweetcorpus %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation))
postDTMatrix <- DocumentTermMatrix(postprocessedcorpus)
postSparseDTMatrix <- removeSparseTerms(postDTMatrix, .99)
posttidymatrix <- tidy(postDTMatrix)
postpopterms <- posttidymatrix %>%
group_by(term) %>%
summarize(frequency = sum(count)) %>%
arrange(desc(frequency))
postpopterms[1:20,1:2]
pretweetcorpus <- Corpus(VectorSource(as.vector(PreTwitterData$text)))
preprocessedcorpus <- pretweetcorpus %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(stemDocument), language = "english") %>%
tm_map(content_transformer(removePunctuation))
preDTMatrix <- DocumentTermMatrix(preprocessedcorpus)
preSparseDTMatrix <- removeSparseTerms(preDTMatrix, .99)
inspect(preSparseDTMatrix[1:10,1:10])
pretidymatrix <- tidy(preDTMatrix)
prepopterms <- pretidymatrix %>%
group_by(term) %>%
summarize(frequency = sum(count)) %>%
arrange(desc(frequency))
prepopterms[1:20,1:2]
hashtagcorpus <- Corpus(VectorSource(as.vector(TwitterData$text)))
hashtag <- function(x) gsub("[^#[:alnum:][:space:]]", "", x)
htcorpus2 <- tm_map(hashtagcorpus, content_transformer(hashtag)) %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(content_transformer(tolower))
htDTMatrix <- DocumentTermMatrix(htcorpus2)
tidyhash <- tidy(htDTMatrix)
popht <- tidyhash %>%
group_by(term) %>%
summarize(frequency = sum(count)) %>%
arrange(desc(frequency))
hashtagdaddy <- subset(popht, grepl("#", term))
hashtagdaddy[1:5, 1:2]
DateHashtag <- tidyhash %>%
subset(term == '#maga' | term == '#trump2016'
| term == '#celebapprentice' | term == '#celebrityapprentice'
| term == '#makeamericagreatagain')
x <- 1:17200
TwitterData$document <- x
TwitterData$document <- as.character(TwitterData$document)
final <- inner_join(DateHashtag, TwitterData, by = 'document')
final <- final[, c('document', 'term', 'created_at', 'count')]
final$date <- as.Date(final$created_at, '%m-%d-%Y')
final$month <- format(final$date, '%Y-%m')
maga <- final %>%
group_by(month, term) %>%
summarise(frequency = sum(count))
maga <- arrange(maga, month)
ggplot(data=maga, aes(x=month, y=frequency, group = term, colour = term)) +
geom_line() +
geom_point(size=4, shape=21, fill="white") + coord_flip()
DateHashtag <- tidyhash %>%
subset(term == '#maga' | term == '#trump2016'
| term == '#celebapprentice' | term == '#celebrityapprentice'
| term == '#makeamericagreatagain')
x <- 1:17200
TwitterData$document <- x
TwitterData$document <- as.character(TwitterData$document)
final <- inner_join(DateHashtag, TwitterData, by = 'document')
final <- final[, c('document', 'term', 'created_at', 'count')]
final$date <- as.Date(final$created_at, '%m-%d-%Y')
final$month <- format(final$date, '%Y-%m')
maga <- final %>%
group_by(month, term) %>%
summarise(frequency = sum(count))
maga <- arrange(maga, month)
ggplot(data=maga, aes(x=month, y=frequency, group = term, colour = term)) +
geom_line() +
geom_point(size=4, shape=21, fill="white") + coord_flip()
ggplot(data=maga, aes(x=month, y=frequency, group = term, colour = term)) +
geom_line() +
geom_point(size=4, shape=21, fill="white") + coord_flip()
ggplot(data=maga, aes(x=month, y=frequency, group = term, colour = term)) +
geom_line() +
geom_point(size=4, shape=21, fill="white") + coord_flip()
ggplot(data=maga, aes(x=month, y=frequency, group = term, colour = term)) +
geom_line() +
geom_point(size=4, shape=21, fill="white") + coord_flip()
Crooked <- TwitterData
Crooked <- select(Crooked, c(text, created_at)) %>%
unnest_tokens(bigram, text, token='ngrams', n=2)
Crooked$date <- as.Date(Crooked$created_at, '%m-%d-%Y')
Crooked$month <- format(Crooked$date, '%Y-%m')
Crooked <- subset(Crooked, bigram=='crooked hillary')
for (i in 1:nrow(Crooked)) {
Crooked$count[i] =1
}
finalcrooked <- Crooked %>%
group_by(month) %>%
summarise(frequency = sum(count))
ggplot(data=finalcrooked, aes(x=month, y=frequency)) +
geom_col(color='blue', fill='red')
View(Crooked)
Crooked %>%
complete(month = seq.Date(min(month), max(month), by="month"))
View(Crooked)
library(zoo)
install.packages('zoo')
library(zoo)
all_dates = seq(as.Date(as.yearmon(min(Crooked$month))), as.Date(as.yearmon(max(Crooked$month))), by="month")
posts_by_date_clean = merge(data.frame(date = all_dates),
posts_by_date,
by.x='date',
by.y='year_mon',
all.x=T,
all.y=T)
all_dates = seq(as.Date(as.yearmon(min(Crooked$month))), as.Date(as.yearmon(max(Crooked$month))), by="month")
posts_by_date_clean = merge(data.frame(date = all_dates),
Crooked,
by.x='month',
by.y='month',
all.x=T,
all.y=T)
View(finalcrooked)
all_dates = seq(as.Date(as.yearmon(min(finalcrooked$month))), as.Date(as.yearmon(max(finalcrooked$month))), by="month")
all_dates = seq(as.Date(as.yearmon(min(finalcrooked$month))), as.Date(as.yearmon(max(finalcrooked$month))), by="month")
posts_by_date_clean = merge(data.frame(date = all_dates),
Crooked,
by.x='date',
by.y='month',
all.x=T,
all.y=T)
data.frame(date = all_dates
date=  data.frame(all_dates)
date = as.data.frame(all_dates)
View(date)
date$month <- format(date$all_dates, '%Y-%m')
all_dates = seq(as.Date(as.yearmon(min(finalcrooked$month))), as.Date(as.yearmon(max(finalcrooked$month))), by="month")
date = as.data.frame(all_dates)
date$month <- format(date$all_dates, '%Y-%m')
final <- inner_join(date, finalcrooked, by = 'month')
date <- inner_join(date, finalcrooked, by = 'month')
View(final)
all_dates = seq(as.Date(as.yearmon(min(finalcrooked$month))), as.Date(as.yearmon(max(finalcrooked$month))), by="month")
date = as.data.frame(all_dates)
date$month <- format(date$all_dates, '%Y-%m')
posts_by_date_clean = merge(data.frame(date = all_dates),
finalcrooked,
by.x='date',
by.y='month',
all.x=T,
all.y=T)
posts_by_date_clean = merge(date,
finalcrooked,
by.x='month',
by.y='month',
all.x=T,
all.y=T)
View(posts_by_date_clean)
posts_by_date_clean$frequency[is.na(posts_by_date_clean$frequency)] = 0
ggplot(data=finalcrooked, aes(x=month, y=frequency)) +
geom_col(color='blue', fill='red')
ggplot(data=posts_by_date_clean, aes(x=month, y=frequency)) +
geom_col(color='blue', fill='red')
View(maga)
all_dates2 = seq(as.Date(as.yearmon(min(maga$month))), as.Date(as.yearmon(max(maga$month))), by="month")
date2 = as.data.frame(all_dates2)
date2$month <- format(date2$all_dates, '%Y-%m')
View(date2)
posts_by_date_clean2 = merge(date2,
maga,
by.x='month',
by.y='month',
all.x=T,
all.y=T)
View(posts_by_date_clean2)
posts_by_date_clean2$frequency[is.na(posts_by_date_clean2$frequency)] = 0
ggplot(data=maga, aes(x=month, y=frequency, group = term, colour = term)) +
geom_line() +
geom_point(size=4, shape=21, fill="white") + coord_flip()
posts_by_date_clean2$term[is.na(posts_by_date_clean2$term)] = '#trump2016'
ggplot(data=maga, aes(x=month, y=frequency, group = term, colour = term)) +
geom_line() +
geom_point(size=4, shape=21, fill="white") + coord_flip()
posts_by_date_clean2 = merge(date2,
maga,
by.x='month',
by.y='month',
all.x=T,
all.y=T)
posts_by_date_clean2$frequency[is.na(posts_by_date_clean2$frequency)] = 0
ggplot(data=posts_by_date_clean2, aes(x=month, y=frequency, group = term, colour = term)) +
geom_line() +
geom_point(size=4, shape=21, fill="white") + coord_flip()
posts_by_date_clean2$term[is.na(posts_by_date_clean2$term)] = '#celebapprentice'
ggplot(data=posts_by_date_clean2, aes(x=month, y=frequency, group = term, colour = term)) +
geom_line() +
geom_point(size=4, shape=21, fill="white") + coord_flip()
posts_by_date_clean2$term[is.na(posts_by_date_clean2$term)] = '#trump2016'
ggplot(data=posts_by_date_clean2, aes(x=month, y=frequency, group = term, colour = term)) +
geom_line() +
geom_point(size=4, shape=21, fill="white") + coord_flip()
posts_by_date_clean2 = merge(date2,
maga,
by.x='month',
by.y='month',
all.x=T,
all.y=T)
posts_by_date_clean2$frequency[is.na(posts_by_date_clean2$frequency)] = 0
posts_by_date_clean2$term[is.na(posts_by_date_clean2$term)] = '#trump2016'
ggplot(data=posts_by_date_clean2, aes(x=month, y=frequency, group = term, colour = term)) +
geom_line() +
geom_point(size=4, shape=21, fill="white") + coord_flip()
ggplot(data=posts_by_date_clean2, aes(x=month, y=frequency, group = term, colour = term)) +
geom_line() +
geom_point(size=4, shape=21, fill="white") + coord_flip() +
coord_cartesian(ylim=c(-1,100))
ggplot(data=posts_by_date_clean2, aes(x=month, y=frequency, group = term, colour = term)) +
geom_line() +
geom_point(size=4, shape=21, fill="white") + coord_flip() +
coord_cartesian(xlim=c(-1,100))
ggplot(data=posts_by_date_clean2, aes(x=month, y=frequency, group = term, colour = term)) +
geom_line() +
geom_point(size=4, shape=21, fill="white") +
scale_y_continuous(limits = c(1, 100)) + coord_flip()
