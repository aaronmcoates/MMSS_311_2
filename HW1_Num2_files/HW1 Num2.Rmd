---
title: "HW1 Num2"
author: "Aaron Coates"
date: "4/17/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd("~/Documents/GitHub/MMSS_311_2")
wid <- read.csv("/Users/aaroncoates/Downloads/widget_data.csv")

library(dplyr)
library(ggplot2)
library(glmnet)
library(broom)
library(aod)
```

First, I plot the dependent variable, widget performance.
```{r, echo=FALSE, fig.align='center'}
ploty <- ggplot(wid, aes(y)) + geom_histogram(binwidth = 1, color="blue", aes(fill = ..x..)) +
  labs(title ="Distribution of Widget Performance", 
  x = "Widget Performance", y = "Frequency") + scale_fill_continuous(guide=FALSE) +
  scale_x_continuous(breaks = seq(round(min(wid$y),0), round(max(wid$y),0), by = 1)) +
  scale_y_continuous(breaks = seq(0:15))
ploty
```

I perform the ridge regression, with lambda ranging from 1/100 to 100.
```{r}
grid=10^seq(2,-2,length=100)
ridge <- glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=0, lambda=grid)
```

```{r}
ridge1 <- tidy(ridge)
```

Now, I will plot the coefficients as lambda changes.
```{r, echo=FALSE, fig.align='center'}
ggplot(data=ridge1, aes(x=log10(lambda), y=estimate, colour=term)) + 
  geom_line() + labs(title ="Ridge Coefficients", 
  x = "Log(Lambda)", y = "Coefficient Estimate") +
  coord_cartesian(xlim = c(-2,2), ylim = c(-1, 1))
```

Now, I will crossvalidate the data and plot the results.
```{r}
crossval <- cv.glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=0, lambda=grid)
```
```{r, echo=FALSE}
plot(crossval)
```

The results below show the lambda value that minimizes MSE. 
The coefficients are given.
```{r}
crossval$lambda.min
coef(crossval, s = "lambda.min")
```

Now, I will perform the same steps for the LASSO.
```{r}
grid=10^seq(2,-2,length=100)
lasso <- glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=1, lambda=grid)
```

```{r}
lasso1 <- tidy(lasso)
```

Below, I will plot the coefficient estimates as lambda changes.
```{r, echo=FALSE, fig.align='center'}
ggplot(data=lasso1, aes(x=lambda, y=estimate, colour=term)) + 
  geom_line() + coord_cartesian(xlim = c(0,2), ylim = c(-1, 1)) +
  labs(title ="LASSO Coefficients", x = "Lambda", y = "Coefficient Estimate")
```

The graph below zooms in on the clustered lines above.
```{r, echo=FALSE, fig.align='center'}
ggplot(data=lasso1, aes(x=lambda, y=estimate, colour=term)) + 
  geom_line() + coord_cartesian(xlim = c(0,.5), ylim = c(-.5, .5)) + 
  labs(title ="LASSO Coefficients", x = "Lambda", y = "Coefficient Estimate")
```

Now, I will crossvalidate and plot the results.
```{r}
crossvallasso <- cv.glmnet(x = as.matrix(wid[, -1]), y = wid$y, alpha=1, lambda=grid)
```
```{r, echo=FALSE}
plot(crossvallasso)
```

Below is the lambda value that minimizes MSE. The coefficient estimates are given.
```{r}
crossvallasso$lambda.min
coef(crossvallasso, s = "lambda.min")
```

As can be seen, the ridge regression shrinks the coefficient on every independent variable, while the LASSO regression selects the most important independent variables, while the less important coefficients shrink to zero. It seems as though the LASSO regression is the more effective regression technique in this instance. This is because the LASSO helps to eliminate less important variables. This is especially helpful since the question states that we do not know which specific data are relevant for the outcome we are measuring.
